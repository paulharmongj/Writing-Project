---
title: "Demystifying the Carnegie Classifications"
author: "Paul Harmon"
date: "April 17, 2017"
output:
  beamer_presentation: 
    toc: true
    theme: "AnnArbor"
    colortheme: "crane"
    fonttheme: "structurebold"
  ---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r setup2 the streets, include = FALSE}
setwd("C:/Users/Paul/Documents/Carnegie Classifications")
#updated file
cc2015.full <- read.csv("CC2015data.csv", header = TRUE, as.is = TRUE)

cc2015 <- cc2015.full[(cc2015.full$BASIC2015>14&cc2015.full$BASIC2015<18),]
cc2015$BASIC2015 <- factor(cc2015$BASIC2015)
#ranked

frank <- function(x){rank(x, ties.method = "first")}
lrank <- function(x){rank(x, ties.method = "last")}
minrank <- function(x){rank(x, ties.method = "min")}
maxrank <- function(x){rank(x, ties.method = "max")}

cc2015Ps<-
na.omit(cc2015[,c("NAME","BASIC2010","BASIC2015","FACNUM","HUM_RSD","OTHER_RSD","SOCSC_RSD","STEM_RSD","PDNFRSTAFF","S.ER.D","NONS.ER.D")])

#calculate the ranked data
cc2015.r <- data.frame(cc2015Ps[,1:3],sapply(cc2015Ps[,-c(1:3)],minrank)) 

cc2015percap <- cc2015Ps[,c("PDNFRSTAFF","S.ER.D","NONS.ER.D")]/cc2015Ps$FACNUM
cc2015percap.r<-data.frame(sapply(cc2015percap,minrank))
AGcc <- function(x){
  #rank the data
  ranked <- data.frame(x[,1:3],sapply(x[,-c(1:3)],minrank)) 
  #get pc's
  pca.ranked <- prcomp(ranked[,-c(1:4)], scale = TRUE)
  summary <- summary(pca.ranked)
  standard.score <- scale(pca.ranked$x[,1], scale = TRUE, center = TRUE)
  #needs to return the standardized scores
  return(list(scorez = standard.score, sum =summary))
}
#function for percap
PCcc <- function(x){
  #rank the data
  ranked.dat <- data.frame(sapply(x,minrank)) 
  #get pc's
  pc.ranked <- prcomp(ranked.dat, scale = TRUE)
  summary <- summary(pc.ranked)
  standard.score <- scale(pc.ranked$x[,1], scale = TRUE, center = TRUE)
  return(list(scorez = standard.score, sum = summary))
}
a <- which(cc2015Ps$NAME == "Montana State University")
```


## Introduction: What are the Carnegie Classifications?
+ The Carnegie Classifications are a metric by which like institutions can be compared. 
+ Three Classifications of Doctorate-Granting Universities
+ Many different classifications of Bachelor's, Associates-only institutions
_In 2015, the classifcations were used on more than 4600 institutions, everything from Stanford University to the Golf Academy of America._ 


![Montana State (R-2), Stanford (R-1), Boise State(R-3): ](C:\Users\Paul\Documents\Carnegie Classifications\Colleges.png)

## Montana State University: A History
+ Montana State had been classified as R-1: "Very High Research Activity" in 2005 and 2010
+ In 2015, Montana State moved to R-2: "High Research Activity"
+ MSU has always been high on the per-capita index; however, we do not produce as many PhDs as larger R-1 schools

![2015 Carnegie Classifications](C:\Users\Paul\Documents\CArnegie Classifications\2015 Carnegie Classifications.jpeg)



##Montana State's Nearest Neighbors
+ The system is designed for __CLASSIFICATION__ of institutions, not rankings!
+ Institutions near each other can be considered a "peer group" of schools
Montana State's peer group:

![Nearest Institutions to Montana State](C:\Users\Paul\Documents\CArnegie Classifications\Nearest Neighbors.jpeg)

## Principal Components Analysis
__Goal__: To reduce _p_ predictors into _k_ components via eigenvalue decomposition.
PCA can be done on unscaled raw data or on a scaled covariance matrix.

Given p predictors $x_1, x_2,...x_p$, we can generate via an eigenvalue decomposition of __X__ a set of p new variables $y_1,y_2,...,y_n$. The $y$'s are ordered so that $y_1$ explains the most variation in the underlying $x$'s, and $y_p$ the least. 

__Scores__: The new set of covariates. These are functions (weighted averages) of the old covariates. 
__Loadings__: The loadings give the formula used to calculate the scores from the original covariates. 

_But how do we do dimension reduction?_ Since we know how much variation in $x$ is explained by each $y$-score, we can make a new factor matrix of some subset of the scores. 
__The Carnegie Classifications use only the first score from each PCA run.__ 



## How are the Classifications Calculated?
The classifications are calculated based on two indices of institutional output. The first is based on a weighted average of the number of PhDs awarded by the institution; the second is based on a per-capita measurement of research expenditures and research staff. 
__Aggregate Index:__
$$Ag.Index_{i}  = HumanitiesPhD_{i} + StemPhD_{i} + SocialSciencePhD_{i} + OtherPhD_{i}  + StemExpenditures_{i} + NonStemExpenditures_{i} + ResearchStaff_{i} $$ 
__Per Capita Index:__ 
$$PC.Index_{i} = \frac{ResearchStaff_{i} + StemExpenditures_{i} + NonStemExpenditures_{i}}{FacultySize_{i}} $$ 




## Replicating the Classifications
The scores were calcluated using the following methods:
<div class="columns-2">
  - Rank each instution on the covariates
  - Calculate Principal Component Scores for each index
  - Plot the first PC score from the aggregate index vs the first PC score from the per-capita index
</div>
```{r classplot, include = TRUE}
#plot the two: more of a final plot
percap <- PCcc(cc2015percap)
ag <- AGcc(cc2015Ps)
plot(ag$scorez, -percap$scorez, col = cc2015Ps$BASIC2015, pch = 20, xlab = "Aggregate Index", ylab = "Per Capita Index", main = "Scaled Scores")
points(ag$scorez[a],-percap$scorez[a],col = "blue", pch = 20)

```



## Methods for Ties
Because the scores are based on ranks, __not__ the original data. Since most of the variables used are counts, many schools will be tied, especially for the aggregate index. R calculates ties in several ways:
<div class="columns-2">
- Average (default): If the first 3 schools are tied, each would get rank 1.5. 
- Minimum: If the first 3 schools are tied, each would get rank 1.
- Maximum: If the first 3 schools are tied, each would get rank 3.
- First: If the first 3 schools are tied, permute 1 to 3 to get rank. 
- Last: If the first 3 schools are tied, permute 3 to 1 to get rank. 
</div>

##Methods for Ties:



## Where were lines drawn?

## Unstandardized Scores

## How do we change?

## Single Metric Changes: Aggregate Index

## Single Metric Changes: 

## Next Steps: 
The Carnegie Classifications were based on a number of seemingly arbitrary choices. Why use the minimum rank instead of the average? Why un-scale prior to drawing the lines between groups? Why draw lines where they drew them?
Rather than clustering on the first PC score for two indices, why not include the first two PC scores?
+





## Slide with Plot
Text goes here I think
```{r pressure}
plot(pressure)
```


## References 
